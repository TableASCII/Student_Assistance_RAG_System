{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uQx1bcKS24s1"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install faiss-cpu\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lBfmGmXY1Zem"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "8SucL-Lx5vla"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем и инициализируем обученную retrieval-модель"
      ],
      "metadata": {
        "id": "l2tdRH9W-6T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/e5-custom-trained.zip"
      ],
      "metadata": {
        "id": "G5x31jVR9EVg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "CSFJwhID7cAN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Инициализация модели\n",
        "model = SentenceTransformer(\"/content/content/e5-custom-trained\")"
      ],
      "metadata": {
        "id": "NbX7H7BK50Vf",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Создание векторной БД для семантического поиска\n",
        "Создаем векторное хранилище документов с использованием FAISS, которое обеспечивает эффективный поиск текстовых фрагментов по семантической близости.\n",
        "\n",
        "Объединяем тренировочную и валидационную выборки, извлекаем из них текстовые фрагменты и добавляем префикс `passage: `, чтоб E5 лучше понимала контекст.\n",
        "\n",
        "После этого вычисляем эмбеддинги для обработанных текстов. Это нужно для вычисления близости эмбеддингов запроса пользователя и имеющимися документами в БД.\n",
        "\n",
        "И инициализируем FAISS-индекс."
      ],
      "metadata": {
        "id": "oF7Jt7cFWCYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "train_data_df = pd.read_csv(\"/content/train_data.csv\")\n",
        "val_data_df = pd.read_csv(\"/content/val_data.csv\")\n",
        "documents_df = pd.concat([train_data_df, val_data_df], ignore_index=True)\n",
        "documents = documents_df[\"positive\"].tolist()\n",
        "\n",
        "prefixed_docs = [f\"passage: {doc}\" for doc in documents]\n",
        "embeddings = model.encode(prefixed_docs, normalize_embeddings=True)\n",
        "# Создание FAISS индекса\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "faiss.write_index(index, \"e5_faiss_index.bin\")"
      ],
      "metadata": {
        "id": "1vmuMqbZ520B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция `semantic_search` выполняет семантический поиск релевантных документов в векторной базе по пользовательскому запросу."
      ],
      "metadata": {
        "id": "KMyDtb1nB8bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция поиска\n",
        "def semantic_search(query, top_k=12):\n",
        "    prefixed_query = f\"query: {query}\"\n",
        "    query_embedding = model.encode([prefixed_query], normalize_embeddings=True)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Убираем дубликаты из-за того, что на один абзац формируется 3 ответа\n",
        "    seen = set()\n",
        "    unique_results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        doc = documents[idx]\n",
        "        if doc not in seen:\n",
        "            seen.add(doc)\n",
        "            unique_results.append((doc, float(dist)))\n",
        "\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "PLuYUg7A6dsE",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Отправка запроса к LLM\n",
        "Ретривер готов и теперь осталось отправить запрос к LLM, для генерации ответа по найденным релевантным документам через платформу OpenRouter.\n",
        "\n",
        "###Архитектура RAG:\n",
        "На переданнный вопрос ретривер находит релевантные документы, передает их вместе с промптом в LLM, которая генерирует финальный ответ.\n",
        "\n",
        "Задача LLM - сгенерировать ответ на заданный вопрос, используя информацию, найденную ретривером."
      ],
      "metadata": {
        "id": "Ec2D_GMuCOw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "lHxQ3lEpGmXy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "1qwds5tB4drm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client7 = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get('API_KEY7')\n",
        "  )"
      ],
      "metadata": {
        "id": "f3HLnfmtGupK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question):\n",
        "    # Ищем релевантные абзацы\n",
        "    search_results = semantic_search(question, top_k=12)\n",
        "    if not search_results:\n",
        "        return \"Не удалось найти подходящую информацию\"\n",
        "\n",
        "    best_paragraph, score = search_results[0]\n",
        "    best_paragraph1, score1 = search_results[1]\n",
        "    best_paragraph2, score2 = search_results[2]\n",
        "\n",
        "\n",
        "    # Промпт\n",
        "    prompt = f\"\"\"\n",
        "    Задача:\n",
        "    Найди в предоставленных данных ответ на вопрос пользователя. Затем сформулируй ответ на вопрос пользователя.\n",
        "    Ответ должен звучать как экспертное утверждение, без упоминания источников, данных или контекста. Предоставь максимум информации используя имеющиеся данные.\n",
        "    Информации может быть мало, но даже в таком случае нужно сформулировать ответ на вопрос пользователя.\n",
        "\n",
        "    Вопрос пользователя: {question}\n",
        "    Данные для ответа:\n",
        "    1. {best_paragraph}\n",
        "    2. {best_paragraph1}\n",
        "    3. {best_paragraph2}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Запрос к LLM\n",
        "    completion = client7.chat.completions.create(\n",
        "      extra_body={},\n",
        "      model=\"meta-llama/llama-4-maverick:free\",\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "            {\n",
        "              \"type\": \"text\",\n",
        "              \"text\": prompt\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    # Обработка ответа\n",
        "    if completion and completion.choices:\n",
        "        content = completion.choices[0].message.content\n",
        "        return f\"\"\"\n",
        "        Ответ: {content}\\n\\n\n",
        "\n",
        "        \"\"\"\n",
        "    else:\n",
        "        print(\"Ошибка: LLM не вернул ответ\")\n"
      ],
      "metadata": {
        "id": "lXi_U_ajDvqi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример работы ретривера:"
      ],
      "metadata": {
        "id": "qqr9DiKQFi_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования\n",
        "results = semantic_search(\"Какие мероприятия в политехе?\")\n",
        "for doc, score in results:\n",
        "    print(f\"Score: {score:.4f} | {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRKMRJFNE2r9",
        "outputId": "41b475bc-9039-407a-d119-cde610d75f37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.4839 | самое теплое мероприятие, которое как нельзя кстати проходит\n",
            "\n",
            "- «Леденец» в холодную зимнюю пору! Настоящее катание на коньках, конкурсы, призы уверены, ты уже ждешь!\n",
            "Score: 0.4328 | И это далеко не всё! В течение всего курса каждый сможет найти себе мероприятие по душе!\n",
            "Score: 0.4278 | Во время летних каникул ребят ждет отдых в спортивно-оздоровительном лагере «Ждановец». Руководство политеха активно взаимодействует со студенческим активом и студенческими объединениями.\n",
            "Score: 0.4269 | Тогда мы ждем именно тебя! У нас своя репетиционная точка, мы располагаем большим количеством инструментов и выступаем на главных мероприятиях Политеха и Ждановца, а также за их пределами.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Создание графического интерфейса\n",
        "Для обеспечения удобного взаимодействия пользователей с RAG-системой реализован веб-интерфейс с использованием библиотеки `gradio`."
      ],
      "metadata": {
        "id": "g8Lrx58TFuQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "4QpL0K-i3MTk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_answer,\n",
        "    inputs=gr.Textbox(label=\"Ваш вопрос\", placeholder=\"Введите ваш вопрос здесь\"),\n",
        "    outputs=gr.Textbox(label=\"Результаты поиска\", lines=10),\n",
        "    title=\"Генерация ответов\",\n",
        "    description=\"Введите вопрос и получите релевантный ответ\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "9KO1IjMY3c9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}